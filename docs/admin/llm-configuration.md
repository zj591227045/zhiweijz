# LLM 配置管理

本系统支持两种LLM配置模式，用户可以根据需求选择合适的配置方式。

## 配置模式对比

### 1. 全局 LLM 配置 (`/admin/llm`)

**适用场景：**
- 简单的AI服务需求
- 单一提供商足够满足使用需求
- 不需要故障转移和负载均衡功能

**功能特点：**
- ✅ 配置单一LLM提供商（OpenAI、硅基流动、自定义等）
- ✅ 设置全局温度、最大Token等参数
- ✅ 配置全局Token使用限额
- ✅ 测试提供商连接
- ✅ 简单易用，快速配置

**配置存储：**
- 存储在 `system_config` 表中
- 使用 `llm_global_*` 配置键
- 影响所有用户的默认AI服务

### 2. 多提供商 LLM 管理 (`/admin/multi-provider-llm`)

**适用场景：**
- 需要高可用性和稳定性
- 多个LLM提供商可供选择
- 需要故障转移和负载均衡功能
- 企业级应用场景

**功能特点：**
- ✅ 配置多个LLM提供商实例
- ✅ 设置提供商优先级（数字越小优先级越高）
- ✅ 自动故障转移（高优先级失败时自动切换到低优先级）
- ✅ 负载均衡策略（轮询、加权、随机）
- ✅ 实时健康监测（通过/models API检查）
- ✅ 提供商模板快速配置
- ✅ 详细的状态监控和错误报告

**配置存储：**
- 存储在 `system_config` 表中，键为 `llm_multi_provider_config`
- JSON格式存储完整的多提供商配置
- 影响所有全局LLM请求

## 配置优先级和冲突处理

### 优先级规则

1. **多提供商模式优先**：当多提供商配置启用且有可用提供商时，系统使用多提供商配置
2. **全局配置回退**：当多提供商模式未启用或无可用提供商时，系统回退到全局LLM配置
3. **用户自定义配置独立**：用户个人的AI服务配置不受全局配置影响

### 配置状态说明

- **🟢 多提供商模式**：多提供商配置已启用，有活跃提供商
- **🟡 单提供商模式**：使用全局LLM配置，多提供商模式未启用
- **🔴 无配置**：两种模式都未正确配置

### 避免冲突的建议

1. **明确使用场景**：根据业务需求选择合适的配置模式
2. **不要同时依赖**：启用多提供商模式时，全局LLM配置将被忽略
3. **测试配置**：配置完成后进行连接测试，确保服务正常
4. **监控状态**：定期检查配置状态和提供商健康状况

## 配置建议

### 简单部署
```
推荐使用：全局 LLM 配置
- 配置一个主要的LLM提供商
- 设置合适的参数和Token限额
- 定期测试连接状态
```

### 企业部署
```
推荐使用：多提供商 LLM 管理
- 配置2-3个LLM提供商作为备选
- 设置合理的优先级（主要提供商优先级1，备用提供商优先级2-3）
- 启用故障转移和健康检查
- 选择合适的负载均衡策略
```

## 用户自定义AI服务

**独立性说明：**
- 用户个人的AI服务配置（在用户设置中）与全局配置完全独立
- 用户可以配置自己的OpenAI API Key等信息
- 用户自定义配置优先级最高，不受全局配置影响
- 当用户没有自定义配置时，才使用全局配置

**职责分工：**
- **全局配置**：为所有用户提供默认的AI服务
- **用户配置**：用户个人的专属AI服务设置
- **多提供商配置**：增强全局服务的可靠性和性能

## API 设计

### 配置检查顺序
```
1. 检查用户是否有自定义AI配置
2. 如有用户配置，使用用户配置
3. 如无用户配置，检查多提供商配置
4. 如多提供商启用且有可用提供商，使用多提供商配置
5. 否则回退到全局LLM配置
```

这种设计确保了配置的层次性和灵活性，满足不同场景的需求。 